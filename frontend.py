import streamlit as st
from backend import ask_pablo, read_file
import time
import json
from fpdf import FPDF
import io

st.set_page_config(page_title="Pablo ‚Äì Traitement Automatis√©", page_icon="üï∂Ô∏è")

# -------------------------
# 1. CHARGEMENT DU CONTEXTE (Vos instructions ma√Ætresses)
# -------------------------
try:
    # C'est ici que r√©sident vos instructions (ex: "Extrais les totaux", "Cherche les erreurs", etc.)
    sys_content = read_file("./context.txt")
except:
    sys_content = "Tu es un assistant expert en analyse JSON."

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": sys_content}]

st.title("üï∂Ô∏è Sonalyze - Analyse Automatis√©e")
st.write("Le syst√®me appliquera les instructions de votre `context.txt` √† chaque fichier.")

# -------------------------
# 2. AFFICHAGE HISTORIQUE
# -------------------------
for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

# -------------------------
# 3. UPLOAD ET D√âCLENCHEMENT
# -------------------------
with st.container():
    uploaded_files = st.file_uploader(
        "D√©posez vos fichiers JSON (Traitement s√©quentiel)",
        type=["json"],
        accept_multiple_files=True
    )
    # Le bouton lance directement le processus d√©fini dans le contexte
    send_button = st.button("Lancer le traitement ‚ö°", disabled=(not uploaded_files))

if send_button and uploaded_files:

    progress_bar = st.progress(0)
    status_text = st.empty()
    analyses_partielles = []

    try:
        # --- √âTAPE 1 : ANALYSE FICHIER PAR FICHIER ---
        for i, file in enumerate(uploaded_files):
            file_name = file.name
            status_text.write(
                f"‚öôÔ∏è Application des instructions au fichier {i + 1}/{len(uploaded_files)} : **{file_name}**...")

            # Lecture
            file_content = json.load(file)

            # Minification pour √©conomiser les tokens
            json_str = json.dumps(file_content, separators=(',', ':'), ensure_ascii=False)

            # On coupe si > 120k caract√®res (approx 30k tokens) pour garder de la place pour la r√©ponse
            if len(json_str) > 120000:
                json_str = json_str[:120000] + "... (tronqu√©)"

            # --- C'est ici que la magie op√®re ---
            # On envoie : LE SYSTEM PROMPT + LE FICHIER
            # Le mod√®le va donc ex√©cuter vos ordres sur ce fichier pr√©cis.
            messages_intermediaires = [
                {"role": "system", "content": sys_content},
                {"role": "user", "content": f"Voici le contenu du fichier '{file_name}' √† traiter : {json_str}"}
            ]

            # Appel Backend (Map)
            stream = ask_pablo(chat_history=messages_intermediaires)

            partial_res = ""
            for chunk in stream:
                content = chunk.choices[0].delta.content
                if content:
                    partial_res += content

            analyses_partielles.append(f"--- R√©sultat pour {file_name} ---\n{partial_res}\n")
            progress_bar.progress((i + 1) / len(uploaded_files))

        # --- √âTAPE 2 : CONSOLIDATION FINALE ---
        status_text.write("üìë Consolidation des r√©sultats...")

        # On regroupe toutes les analyses partielles
        global_context = "\n".join(analyses_partielles)

        # On demande au mod√®le de finaliser (si besoin) ou d'afficher le tout
        # On r√©injecte le sys_content pour qu'il garde sa personnalit√©/format de sortie
        final_prompt_content = f"Voici les r√©sultats de l'analyse individuelle de chaque fichier. Compile ou pr√©sente le r√©sultat final conform√©ment √† tes instructions syst√®me :\n\n{global_context}"

        # Ajout √† l'historique visible (User)
        st.session_state.messages.append({"role": "user", "content": "Traitement des fichiers effectu√©."})

        with st.chat_message("assistant"):
            placeholder = st.empty()
            full_response = ""

            # Appel Backend Final (Reduce)
            stream_final = ask_pablo(chat_history=[
                {"role": "system", "content": sys_content},
                {"role": "user", "content": final_prompt_content}
            ])

            for chunk in stream_final:
                content = chunk.choices[0].delta.content
                if content:
                    full_response += content
                    placeholder.write(full_response)
                time.sleep(0.005)

            # Sauvegarde r√©ponse
            st.session_state.messages.append({"role": "assistant", "content": full_response})

        status_text.empty()
        progress_bar.empty()

    except Exception as e:
        st.error(f"Erreur durant le traitement : {e}")
