'''import streamlit as st
from backend import ask_pablo, read_file
import time
import json

st.set_page_config(page_title="Pablo ‚Äì Le Parrain du Chatbot", page_icon="üï∂Ô∏è")

# -------------------------
# 1. INITIALISATION DE LA M√âMOIRE
# -------------------------
if "messages" not in st.session_state:
    try:
        sys_content = read_file("./context.txt")
    except:
        sys_content = "Tu es un assistant expert en analyse de donn√©es JSON."

    st.session_state.messages = [
        {"role": "system", "content": sys_content}
    ]

st.title("üï∂Ô∏è Test - Chatbot Multi-JSON")
st.write("Chargez un ou plusieurs fichiers JSON, puis validez pour lancer l'analyse.")

# -------------------------
# 2. AFFICHAGE DE L‚ÄôHISTORIQUE
# -------------------------
for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            # On tente d'afficher proprement les JSONs
            try:
                # Si le message commence par [ ou {, c'est probablement du JSON
                if msg["content"].strip().startswith(("{", "[")):
                    json_data = json.loads(msg["content"])
                    # On met le JSON dans un expander pour ne pas polluer visuellement le chat
                    with st.expander("Voir le contenu JSON envoy√©"):
                        st.json(json_data)
                else:
                    st.write(msg["content"])
            except:
                st.write(msg["content"])

# -------------------------
# 3. ZONE D'UPLOAD MULTIPLE & VALIDATION
# -------------------------

# Cr√©ation d'un formulaire ou d'une zone conteneur pour regrouper upload + bouton
with st.container():
    # accept_multiple_files=True renvoie une LISTE de fichiers
    uploaded_files = st.file_uploader(
        "Importer vos fichiers JSON",
        type=["json"],
        accept_multiple_files=True
    )

    # Le bouton sert de "d√©clencheur" pour √©viter que le LLM ne parte au quart de tour
    send_button = st.button("Lancer l'analyse üöÄ", disabled=(not uploaded_files))

# -------------------------
# 4. TRAITEMENT LORS DU CLIC
# -------------------------
if send_button and uploaded_files:

    combined_data = []

    # Barre de progression (optionnel, sympa si beaucoup de fichiers)
    progress_bar = st.progress(0)

    try:
        # On boucle sur tous les fichiers upload√©s
        for i, file in enumerate(uploaded_files):
            file_content = json.load(file)
            # On structure les donn√©es pour que le LLM sache quel contenu vient de quel fichier
            combined_data.append({
                "filename": file.name,
                "content": file_content
            })
            progress_bar.progress((i + 1) / len(uploaded_files))

        progress_bar.empty()  # On retire la barre une fois fini

        # Conversion de la liste globale en string JSON
        json_string = json.dumps(combined_data, indent=2, ensure_ascii=False)

        # Ajout √† la m√©moire (c√¥t√© User)
        st.session_state.messages.append({"role": "user", "content": json_string})

        # Affichage imm√©diat dans le chat
        with st.chat_message("user"):
            st.write(f"üìÇ **{len(uploaded_files)} fichiers envoy√©s**")
            with st.expander("D√©tails des donn√©es envoy√©es"):
                st.json(combined_data)

        # -------------------------
        # APPEL AU BACKEND GROQ
        # -------------------------
        with st.chat_message("assistant"):
            placeholder = st.empty()
            full_response = ""

            # Indicateur visuel pendant l'attente
            with st.spinner('Pablo analyse les documents...'):
                stream = ask_pablo(chat_history=st.session_state.messages)

            # Lecture du stream
            for chunk in stream:
                content = chunk.choices[0].delta.content
                if content:
                    full_response += content
                    placeholder.write(full_response)
                time.sleep(0.005)

            # Sauvegarde de la r√©ponse assistant
            st.session_state.messages.append(
                {"role": "assistant", "content": full_response}
            )

    except json.JSONDecodeError:
        st.error("L'un des fichiers n'est pas un JSON valide.")
    except Exception as e:
        st.error(f"Erreur lors du traitement : {e}")

    # Optionnel : Rerun pour nettoyer l'uploader visuellement (si souhait√©)
    # st.rerun()'''
import streamlit as st
from backend import ask_pablo, read_file
import time
import json

st.set_page_config(page_title="Pablo ‚Äì Analyste S√©quentiel", page_icon="üï∂Ô∏è")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": "Tu es un expert JSON."}]

st.title("üï∂Ô∏è Pablo - Analyseur de gros volumes")
st.info("Mode 'S√©quentiel' actif : Les fichiers sont trait√©s un par un pour √©viter la saturation m√©moire.")

# --- AFFICHAGE HISTORIQUE ---
for msg in st.session_state.messages:
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

# --- UPLOAD ---
with st.container():
    uploaded_files = st.file_uploader("Fichiers JSON", type=["json"], accept_multiple_files=True)
    user_question = st.text_input("Quelle est votre question sur ces fichiers ?", "Fais-moi un r√©sum√© global.")
    send_button = st.button("Analyser les fichiers üöÄ", disabled=(not uploaded_files))

if send_button and uploaded_files:

    # 1. Barre de progression
    progress_bar = st.progress(0)
    status_text = st.empty()

    summaries = []  # On va stocker les r√©sum√©s de chaque fichier ici

    # 2. BOUCLE S√âQUENTIELLE (Le secret pour ne pas crasher)
    try:
        for i, file in enumerate(uploaded_files):
            file_name = file.name
            status_text.write(f"üîç Analyse du fichier {i + 1}/{len(uploaded_files)} : **{file_name}**...")

            # Lecture du fichier
            file_content = json.load(file)

            # On pr√©pare un mini-prompt pour CE fichier uniquement
            # On tronque √† 40k tokens par s√©curit√© pour laisser de la place √† la r√©ponse
            json_str = json.dumps(file_content, separators=(',', ':'), ensure_ascii=False)[:160000]

            prompt_intermediaire = [
                {"role": "system", "content": "Tu es un extracteur de donn√©es. Analyse le JSON fourni."},
                {"role": "user",
                 "content": f"Voici le fichier {file_name} : {json_str}. \n\n T√ÇCHE : Extrais les informations pertinentes par rapport √† la demande : '{user_question}'. Sois concis."}
            ]

            # Appel API pour ce fichier sp√©cifique
            # Note : On n'utilise pas l'historique global ici pour √©conomiser la m√©moire
            response_stream = ask_pablo(chat_history=prompt_intermediaire)

            file_summary = ""
            for chunk in response_stream:
                content = chunk.choices[0].delta.content
                if content:
                    file_summary += content

            summaries.append(f"--- R√©sum√© de {file_name} ---\n{file_summary}\n")
            progress_bar.progress((i + 1) / len(uploaded_files))

        # 3. SYNTH√àSE FINALE
        status_text.write("üß† Synth√®se de tous les fichiers en cours...")

        # On combine tous les r√©sum√©s interm√©diaires
        final_context = "\n".join(summaries)

        # On construit le prompt final
        final_prompt = f"J'ai analys√© {len(uploaded_files)} fichiers s√©par√©ment. Voici leurs r√©sum√©s :\n\n{final_context}\n\nQUESTION GLOBALE : {user_question}"

        # Ajout √† l'historique visible
        st.session_state.messages.append({"role": "user", "content": final_prompt})

        with st.chat_message("assistant"):
            placeholder = st.empty()
            full_response = ""

            # Appel API final avec le contexte dig√©r√©
            stream = ask_pablo(chat_history=[
                {"role": "system", "content": "Tu es un analyste qui synth√©tise plusieurs rapports."},
                {"role": "user", "content": final_prompt}
            ])

            for chunk in stream:
                content = chunk.choices[0].delta.content
                if content:
                    full_response += content
                    placeholder.write(full_response)
                time.sleep(0.005)

            st.session_state.messages.append({"role": "assistant", "content": full_response})

        status_text.empty()
        progress_bar.empty()

    except Exception as e:
        st.error(f"Une erreur est survenue : {e}")